n_samples: 5

random_text_slicing: 'False'
freeze_temperature_embeddings: 'True'
freeze_temperature_clusters: 'True'

training:
  clip_base_model_name: openai/clip-vit-base-patch32
  clusters_count: 18
  embeddings_dimension: 128
  epochs_count: 1
  learning_rate: 3e-4
  batch_size: 768
  regularization_strength: 1.0
  temperature_clusters: 1.0
  temperature_embeddings: 0.5
  patience: 3

transform:
  size: 224
  scale: 0.2 1.0
  brightness: 0.4
  contrast: 0.4
  saturation: 0.4
  hue: 0.1
  p_color_jitter: 0.8
  p_gray_scale: 0.2
  p_gaussian_blur: 0.5
  sigma: 0.1 2.0

random:
  seed: 42

triples:
  path:
    all: 'data/raw/triples.csv'
    filtered: 'data/processed/triples.csv'

texts:
  path:
    entities: 'data/processed/entities_texts.csv' 

labels:
  ratio:
    pretraining_to_all: 0.2
    validation_to_pretraining: 0.2 
  path:
    all: 'data/interim/labels.csv'
    pretraining:
      all: 'data/interim/pretraining_labels.csv'
      train: 'data/processed/pretraining_train_labels.csv'
      validation: 'data/processed/pretraining_validation_labels.csv'
    clustering:
      all: 'data/processed/clustering_labels.csv'

images:
  captions:
    path: 'data/processed/captions.csv'
  path:
    archive: 'data/raw/images.tar.gz'
    directory: 'data/processed/images'

run_id:
  file_path:
    transformed_image_vs_transformed_image_single_projection: 'models/cluster_trans_img_vs_trans_img_1_proj.txt'
    transformed_image_vs_transformed_image_raw_single_projection: 'models/cluster_trans_img_vs_trans_img_raw_1_proj.txt'
    image_vs_text_single_projection: 'models/img_vs_txt_1_proj.txt'
    transformed_image_vs_text_single_projection: 'models/trans_img_vs_txt_1_proj.txt'
    image_vs_text_raw_double_projection: 'models/image_vs_text_raw_2_proj.txt'
    transformed_image_vs_text_raw_double_projection: 'models/trans_img_vs_txt_raw_2_proj.txt'