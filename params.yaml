training:
  clip_base_model_name: openai/clip-vit-base-patch32
  clusters_count: 10
  embeddings_dimension: 128
  epochs_count: 200
  learning_rate: 3e-4
  batch_size: 64
  regularization_strength: 1.0
  temperature_clusters: 1.0
  temperature_embeddings: 0.5

transform:
  size: 224
  scale: 0.2 1.0
  brightness: 0.4
  contrast: 0.4
  saturation: 0.4
  hue: 0.1
  p_color_jitter: 0.8
  p_gray_scale: 0.2
  p_gaussian_blur: 0.5
  sigma: 0.1 2.0

random:
  seed: 42

triples:
  path:
    all: 'data/raw/triples.csv'
    filtered: 'data/processed/triples.csv'

texts:
  path:
    entities: 'data/processed/entities_texts.csv' 

labels:
  ratio:
    pretraining_to_all: 0.2
    validation_to_pretraining: 0.2 
  path:
    all: 'data/interim/labels.csv'
    pretraining:
      all: 'data/interim/pretraining_labels.csv'
      train: 'data/processed/pretraining_train_labels.csv'
      validation: 'data/processed/pretraining_validation_labels.csv'
    clustering:
      all: 'data/processed/clustering_labels.csv'

images:
  path:
    archive: 'data/raw/images.tar.gz'
    directory: 'data/processed/images'

run_id:
  file_path:
    image_vs_image: 'models/image_vs_image.txt'